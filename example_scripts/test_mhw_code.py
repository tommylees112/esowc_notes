"""
Ufuncs are faster to operate on xarray objects
https://docs.scipy.org/doc/numpy/reference/ufuncs.html

Can run
http://xarray.pydata.org/en/stable/computation.html
"""

from pathlib import Path
import xarray as xr
import numpy as np
import pandas as pd
import xclim
import matplotlib.pyplot as plt

from src.exporters.base import get_kenya
from scripts.eng_utils import select_bounding_box_xarray

data_dir = Path('data')
interim_dir = data_dir / 'interim'
p_dir = interim_dir / 'chirps_preprocessed' / 'chirps_19812019_kenya.nc'
v_dir = interim_dir / 'vhi_preprocessed' / 'vhi_preprocess_kenya.nc'

p = xr.open_dataset(p_dir)
p = p.sortby('time')
v = xr.open_dataset(v_dir)
v = v.sortby('time')

chirps_daily_dir = Path('/soge-home/data_not_backed_up/satellite/chirps_v20/')
daily_dirs = [chirps_daily_dir / file for file in ['chirps-v2.0.2010.days_p05.nc', 'chirps-v2.0.2011.days_p05.nc']]

mf_ds = xr.open_mfdataset(daily_dirs)
ds = xr.open_dataset(daily_dirs[0])

kenya_region = get_kenya()
ds = select_bounding_box_xarray(ds, kenya_region)
mf_ds = select_bounding_box_xarray(mf_ds, kenya_region)

# test for part of a year
ds_ = mf_ds.isel(time=slice(0,-30))
ds_ = ds_.load()


# ------------------------------------------------------------------------------
# Calculate threshold and climatology
# ------------------------------------------------------------------------------
## test threshold comparison
variable = 'precip'
time_period = 'dayofyear'

# get seasonality for doy
clim = ds_.groupby(f'time.{time_period}').mean(dim='time')
# https://stackoverflow.com/a/47103407/9940782
# thresh = ds.groupby('time.dayofyear').reduce(np.nanpercentile, dim='time', q=0.9)
thresh = clim - ds_.groupby(f'time.{time_period}').std(dim='time')





def calculate_threshold(ds, clim, method, time_period, hilo=None, ):
    """
    method : str
        ["q90","q10","std","abs",]
    """
    if method == "q90":
        warnings.warn('this method is currently super slow')
        thresh = ds.groupby(f'time.{time_period}').reduce(np.nanpercentile, dim='time', q=0.9)

    elif method == "q10":
        warnings.warn('this method is currently super slow')
        thresh = ds.groupby(f'time.{time_period}').reduce(np.nanpercentile, dim='time', q=0.1)

    elif method == "std":
        assert hilo is not None, f"If you want to calculate the threshold as std \
            from mean, then have to specify `hilo` = ['low','high']"
        std = ds.groupby(f'time.{time_period}').std(dim='time')
        thresh = clim - std if hilo == 'low' else clim + std

    elif method == "abs":
        assert False, "Not yet implemented the absolute value threshold"
        values = np.ones(ds[variable].shape) * value
        thresh = xr.Dataset(
            {variable: (['time','latitude','longitude'], values)},
            coords={
                'latitude': ds.latitude,
                'longitude': ds.longitude,
                'time': ds.time,
            }
        )

    else:
        assert False, 'Only implemented threshold calculations for \
            ["q90","q10","std","abs",]'

    return thresh


def get_thresh_dataarrays(ds, time_period, hilo=None, method='std'):
    """Get the climatology and threshold xarray objects """
    clim = ds.groupby(f'time.{time_period}').mean(dim='time')
    thresh = calculate_threshold(ds, clim, method=method, time_period=time_period, hilo=hilo,)
    return clim, thresh


time_period='dayofyear'
clim = p.groupby(f'time.{time_period}').mean(dim='time')
calculate_threshold(p, clim, method='std', time_period='dayofyear', hilo='low')
# ------------------------------------------------------------------------------
# Compare values to climatology / threshold
# ------------------------------------------------------------------------------

def create_shape_aligned_climatology(ds, clim, variable, time_period):
    """match the time dimension of `clim` to the shape of `ds` so that can
    perform simple calculations / arithmetic on the values of clim

    Arguments:
    ---------
    ds : xr.Dataset
        the dataset with the raw values that you are comparing to climatology

    clim: xr.Dataset
        the climatology values for a given `time_period`

    variable: str
        name of the variable that you are comparing to the climatology

    time_period: str
        the period string used to calculate the climatology
         time_period = {'dayofyear', 'season', 'month'}

    Notes:
        1. assumes that `latitude` and `longitude` are the
        coord names in ds

    """
    ds[time_period] = ds[f'time.{time_period}']

    values = clim[variable].values
    keys = clim[time_period].values
    # map the `time_period` to the `values` of the climatology (threshold or mean)
    lookup_dict = dict(zip(keys, values))

    # extract an array of the `time_period` values from the `ds`
    timevals = ds[time_period].values

    # use the lat lon arrays (climatology values) in `lookup_dict` corresponding
    #  to time_period values defined in `timevals` and stack into new array
    new_clim_vals = np.stack([lookup_dict[timevals[i]] for i in range(len(timevals))])

    assert new_clim_vals.shape == ds[variable].shape, f"Shapes for new_clim_vals and ds must match! new_clim_vals.shape: {new_clim_vals.shape} ds.shape: {ds[variable].shape}"


    # copy that forward in time
    new_clim = xr.Dataset(
        {variable: (['time','latitude','longitude'], new_clim_vals)},
        coords={
            'latitude': clim.latitude,
            'longitude': clim.longitude,
            'time': ds.time,
        }
    )

    return new_clim
# create_shape_aligned_climatology(ds, clim, variable, time_period)

new_thresh = create_shape_aligned_climatology(ds_, thresh, variable, time_period)
new_clim = create_shape_aligned_climatology(ds_, clim, variable, time_period)

hilo = 'low'
# if hilo == 'low':
    # diff = new_thresh - ds
# elif hilo == 'high':
diff = ds_ - new_thresh

# NOTE: all np.nan values are wiped out with these boolean operators
if hilo == 'low':
    group = (ds_ < new_thresh)[variable]
elif hilo == 'high':
    group = (ds_ > new_thresh)[variable]

from scripts.eng_utils import get_ds_mask

# mask out the true null values (e.g. Sea values)
mask = get_ds_mask(ds)

# converts from bool -> int ([0,1,np.nan])
group = group.where(~mask).astype(bool)

# ------------------------------------------------------------------------------
# calculate exceedences of threshold
# ------------------------------------------------------------------------------

# non_exceedences = (diff[variable] <= 0).where(~mask)
# exceedences = (diff[variable] > 0).where(~mask)
non_exceedences = diff[variable] <= 0
exceedences = diff[variable] > 0

# ------------------------------------------------------------------------------
# Calculate CONSECUTIVE EXCEEDENCES
# http://xarray.pydata.org/en/stable/indexing.html
# ------------------------------------------------------------------------------

events, n_events = ndimage.label(exceed_bool)  # mhw approach

# personal approach -> pandas dataframe
raw_df = ds.precip.to_dataframe(name='precip')
thresh_df = new_thresh.precip.to_dataframe(name='threshold')
exceed_df = group.to_dataframe(name='exceedence')

# xclim approach
# windowed_run_events : Return the number of runs of a minimum length.
# windowed_run_events : Return the number of consecutive true values in array for runs at least as long as given duration.
# longest_run : Return the length of the longest consecutive run of True values.
# .apply(rl.windowed_run_count_ufunc, window=4)
# .apply(rl.windowed_run_count, window=4, dim='time')

from xclim.run_length import rle, windowed_run_events, longest_run

runs = rle(group).load()
wind_events = windowed_run_events(group, window=3).load()
wind_events = wind_events.where(~mask)


fig,ax = plt.subplots()
wind_events.plot(ax=ax)
ax.set_title(f'Number of {variable} events per pixel \
    [{ds_.time.min().values} - {ds_.time.max().values}]\
    \nwindow=3; threshold=mean-1std; hilo={hilo}')

# ------------------------------------------------------------------------------
# Calculate event characteristics
# http://xarray.pydata.org/en/stable/indexing.html
# ------------------------------------------------------------------------------

# where is the max run?
# https://stackoverflow.com/a/40197248/9940782
# https://github.com/pydata/xarray/issues/60
runs.max(dim='time')
# max_runs = runs.where(runs == runs.max(), drop=True).squeeze()
# max_runs = [max_runs.isel(time=i) for i in range(len(max_runs.time.values))]

runs.isel_points(runs=runs.argmax(['latitude','longitude']))

r = runs.to_dataset(name='runs')

# ------------------------------------------------------------------------------
# XClim test
# ------------------------------------------------------------------------------

##Â daily intensity each month
daily_int = xclim.indices._threshold.daily_pr_intensity(ds.precip, freq="QS-DEC")

cdd = xclim.icclim.CDD(ds.precip,freq='QS-DEC')
cdd['season'] = cdd['time.season']
cdd

# ------------------------------------------------------------------------------
# Run for chirps_precip (SUBSET)
# ------------------------------------------------------------------------------
from scripts.eng_utils import get_ds_mask
from xclim.run_length import rle, windowed_run_events

p_dir = data_dir / 'interim' / 'chirps_preprocessed' / 'chirps_19812019_kenya.nc'
p = xr.open_dataset(p_dir)
p = p.sortby('time')

variable = 'precip'
time_period = 'dayofyear'
hilo = 'low'

# work on subset
p = p.sel(time=slice('1980-01-01','1985-01-01'))

# calculate thresholds

pclim, pthresh = get_thresh_dataarrays(p, time_period, hilo=hilo, method='std')
pnew_thresh = create_shape_aligned_climatology(p, pthresh, variable, time_period)
pnew_clim = create_shape_aligned_climatology(p, pclim, variable, time_period)

# calculate exceedences
# NOTE: all np.nan values are wiped out with these boolean operators
if hilo == 'low':
    group = (ds_ < new_thresh)[variable]
elif hilo == 'high':
    group = (ds_ > new_thresh)[variable]

runs = rle(pgroup).load()
wind_events = windowed_run_events(pgroup, window=3).load()


pmask = get_ds_mask(p)
wind_events = wind_events.where(~pmask)


# ------------------------------------------------------------------------------
# Run for chirps_precip ALL DATA
# ------------------------------------------------------------------------------
from pathlib import Path
import xarray as xr
import numpy as np
import pandas as pd


from scripts.eng_utils import get_ds_mask
import xclim.run_length as rl
from xclim.run_length import rle, windowed_run_events, longest_run
from xclim.indices import maximum_consecutive_dry_days

data_dir = Path('data')
p_dir = data_dir / 'interim' / 'chirps_preprocessed' / 'chirps_19812019_kenya.nc'
p = xr.open_dataset(p_dir)
p = p.sortby('time')

variable = 'precip'
time_period = 'dayofyear'
hilo = 'low'

# calculate thresholds
pclim, pthresh = get_thresh_dataarrays(p, time_period, hilo=hilo, method='std')

# create TIME ALIGNED matrices
pclim = create_shape_aligned_climatology(p, pclim, variable, time_period)
pthresh = create_shape_aligned_climatology(p, pthresh, variable, time_period)

# flag exceedences
if hilo == 'low':
    pexceed = (p < pthresh)[variable]
elif hilo == 'high':
    pexceed = (p > pthresh)[variable]


# save the outputs to netcdf
# pclim.to_netcdf(interim_dir / "chirps_preprocessed" / "chirps_19812019_kenya_mean_doy.nc")
# pthresh.to_netcdf(interim_dir / 'chirps_preprocessed' / 'chirps_19812019_kenya_1STD_thresh.nc')
# pthresh = xr.open_dataset('data/interim/chirps_preprocessed/chirps_19812019_kenya_1STD_thresh.nc')
# pclim = xr.open_dataset('data/interim/chirps_preprocessed/chirps_19812019_kenya_mean_doy.nc')
# pclim.to_netcdf(interim_dir / "chirps_preprocessed" / "chirps_19812019_kenya_mean_doy_FULL.nc")
# pthresh.to_netcdf(interim_dir / 'chirps_preprocessed' / 'chirps_19812019_kenya_1STD_thresh_FULL.nc')

def calculate_threshold_exceedences(ds, variable, time_period, hilo, method='std'):
    """ """
    ds = ds.sortby('time')

    # calculate climatology and threshold
    clim, thresh = get_thresh_dataarrays(ds, time_period, hilo=hilo, method=method)

    # make them the same size as the ds variable
    clim = create_shape_aligned_climatology(ds, clim, variable, time_period)
    thresh = create_shape_aligned_climatology(ds, thresh, variable, time_period)

    # flag exceedences
    if hilo == 'low':
        exceed = (ds < thresh)[variable]
    elif hilo == 'high':
        exceed = (ds > thresh)[variable]

    return clim, thresh, exceed

pclim, pthresh, pexceed = calculate_threshold_exceedences(p, variable, time_period, hilo, method='std')


def caclulate_runs_of_exceedences(exceedences):
    """use xclim to calculate the number of consecutive exceedences"""
    assert exceedences.dtype == np.dtype('bool'), f"Expected exceedences to be\
        an array of boolean type. Got {exceedences.dtype}"

    runs = rle(exceedences).load()
    return runs


# mask out the true null values (e.g. Sea values)
pmask = get_ds_mask(p.precip)

# calculate the runs
runs = rle(pexceed).load()

# calculate the number of events that exceed the size of the window
window_size = 3
wind_events = windowed_run_events(pexceed, window=window_size).load()

# calculate the longest run
p_longest = longest_run(pexceed, dim='time').load()
fig,ax = plt.subplots()
p_longest.plot(ax=ax)


# mask out the ocean
wind_events = wind_events.where(~pmask)

# plot the windowed events
fig,ax=plt.subplots()
wind_events.plot(ax=ax)
min_time = pd.to_datetime(p.time.min().values.astype(datetime))
max_time = pd.to_datetime(p.time.max().values.astype(datetime))
date_range = f"{min_time.year}-{min_time.month} - \
    {max_time.year}-{max_time.month}"
ax.set_title(f'Number of {variable} events per pixel \
    [{date_range}]\
    \nwindow={window_size}; threshold=mean-1std; hilo={hilo}')


# recreate maximum_consecutive_dry_days(p) with variable threshold
# What is the MAXIMUM number of consecutive pentads below 1std
group = pexceed
max_seasonal_dry_periods = pexceed.resample(time='QS-NOV').apply(longest_run, dim='time')

# ------------------------------------------------------------------------------
# Random functions
# ------------------------------------------------------------------------------

def recreate_ds(ds):
    """recreate the ds (sometimes helps?)

    Note:
        1. Assumes that only 3 coords (e.g. lat lon time)
        2. Assumes that only 1 variable (`variables[0]`)
    """
    coords = [c for c in ds.coords.keys()]
    variables = [v for v  in ds.variables.keys() if v not in coords]

    # TODO: flexible for loop for each variable (work to `xr.DataArray` first)
    # TODO: need to sort by the size of the coord array
    # coords.sort(key=lambda x,y: cmp(ds[x].shape[0], ds[y].shape[0]))
    # ds[variables[0]].shape  # ORDER BY the shape of the variable

    out = xr.Dataset(
        {variables[0]: (['time','latitude','longitude'], ds[variables[0]])},
        coords = {
            coords[0]: ds[coords[0]],
            coords[1]: ds[coords[1]],
            coords[2]: ds[coords[2]],
        }
    )

    return out



def create_shape_aligned_climatology2(ds, clim, variable, time_period):
    """
    Arguments:
    ---------
    ds : xr.Dataset
        the dataset with the raw values that you are comparing to climatology

    clim: xr.Dataset
        the climatology values for a given `time_period`

    variable: str
        name of the variable that you are comparing to the climatology

    time_period: str
        the period string used to calculate the climatology
         time_period = {'dayofyear', 'season', 'month'}

    Notes:
        1. assumes that `latitude` and `longitude` are the
        coord names in ds
    """
    assert False, "This function only works when the doy/months are STANDARD (e.g. pentads won't work)"
    # create a time_period variable
    ds[time_period] = ds[f'time.{time_period}']

    t = clim[variable].values  # (365, lats, lons)
    doy = clim[time_period].values  # (365,)
    d_doy = ds[time_period].values  # (`time_periods`,)

    time_period_lookup = {'dayofyear':365, 'month':12, 'season':4}
    tps = time_period_lookup[time_period]

    # NOTE: working with pentad data = len(time) == 72
    new_t_vals = t[d_doy % tps, :, :]  # (`time_periods`, lats, lons)

    # create new_clim object with the new_t_vals
    #  (climatology values for all `time_periods`)
    new_clim = xr.Dataset(
        {variable: (['time','latitude','longitude'], new_t_vals)},
        coords={
            'time': ds.time,
            'latitude': clim.latitude,
            'longitude': clim.longitude,
        }
    )

    return new_clim

#
def rle(da, dim='time', max_chunk=1000000):
    # number of timesteps
    n = len(da[dim])
    # time indexes
    i = xr.DataArray(np.arange(da[dim].size), dims=dim).chunk({'time': 1})
    ind = xr.broadcast(i, da)[0].chunk(da.chunks)
    # where is da false? = where the exceedence is NOT met
    b = ind.where(~da)


da = (p.isel(time=slice(0,100)) < 20)


def characteristics():
    # strength = difference from seasonality, difference from threshold
    #  normalised difference from threshold (normalised by difference of threshold:climatology)
    #  absolute value
    mhw_relSeas = temp_mhw - seas_mhw
    mhw_relThresh = temp_mhw - thresh_mhw
    mhw_relThreshNorm = (temp_mhw - thresh_mhw) / (thresh_mhw - seas_mhw)
    mhw_abs = temp_mhw

    # peak intensity

    # Duration

    # MHW Intensity metrics
    mhw["intensity_max"].append(mhw_relSeas[tt_peak])
    mhw["intensity_mean"].append(mhw_relSeas.mean())
    mhw["intensity_var"].append(np.sqrt(mhw_relSeas.var()))
    mhw["intensity_cumulative"].append(mhw_relSeas.sum())
    mhw["intensity_max_relThresh"].append(mhw_relThresh[tt_peak])
    mhw["intensity_mean_relThresh"].append(mhw_relThresh.mean())
    mhw["intensity_var_relThresh"].append(np.sqrt(mhw_relThresh.var()))
    mhw["intensity_cumulative_relThresh"].append(mhw_relThresh.sum())
    mhw["intensity_max_abs"].append(mhw_abs[tt_peak])
    mhw["intensity_mean_abs"].append(mhw_abs.mean())
    mhw["intensity_var_abs"].append(np.sqrt(mhw_abs.var()))
    mhw["intensity_cumulative_abs"].append(mhw_abs.sum())

    return
